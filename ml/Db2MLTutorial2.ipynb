{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"media/Assets&ArchHeader.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Operationalize a Scikit-learn Model with Db2 v11.5\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional scoring of machine learning models with data stored in a database often involves transferring the scoring data to a deployment system where the trained model is stored.  This can cause major challenges such as data transfer costs or data security requirements that can limit the kind or amount of data transferred from the database, such as personal identifiable information. Additionally, hardware and software roadblocks can ultimately prevent the transfer of large amounts of scoring data.\n",
    "\n",
    "In this workshop we will develop a machine learning pipeline externally (i.e. on a local Jupyter notebook) and deploy it to Db2 for in-database scoring via SQL commands.\n",
    "\n",
    "This solution allows users such as business analysts using SQL-based tools and applications to execute the machine learning pipeline by simply querying the Db2 database. \n",
    "\n",
    "This process ultimately keeps your scoring data secure and reduces data transfer latency, required compute power, complexity, and cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisites**: If you are unfamiliar with the Jupyter Notebook environment, it is recommended to have gone through the [An Introduction to Jupyter Notebooks](./An_Introduction_to_Jupyter_Notebooks.ipynb) lab first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider we are on a plane heading towards our destination. We want to determine if the flight will be delayed arriving or not after it leaves the origin airport (binary classification):\n",
    "* A delayed flight will be a flight that arrives late at its destination\n",
    "* If the flight has any delays from its departure, but still arrives to its destination on time, it will not be considered a delayed flight\n",
    "* A *canceled* flight is not a delayed flight as it never left nor arrived to its destination for whatever reason\n",
    "* A *diverted* flight is not a delayed flight as it was diverted from its destination\n",
    "\n",
    "To solve this problem, we will use historical US flight data from 2009-2018. We will then deploy our model to Db2 and make predictions on \"new\" data from 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Connection to Db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "import numpy as np\n",
    "\n",
    "# Model building\n",
    "from sklearn.model_selection import train_test_split # Train-test split\n",
    "from sklearn.impute import SimpleImputer # Missing values imputation\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler # Feature scaling and one-hot encoding\n",
    "from sklearn.linear_model import LogisticRegression # Model\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score # Model evaluation\n",
    "from joblib import dump #Saving deployment assets\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin # For creating transformation pipelines\n",
    "from sklearn.pipeline import Pipeline # For creating transformation pipelines\n",
    "from sklearn.compose import ColumnTransformer # For creating transformation pipelines\n",
    "\n",
    "# Plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(rc={'figure.figsize':(12,8)})\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load the Db2 magic functions and make a connection to our Db2 database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run db2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run connectiondb2ml.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql option maxrows -1 display pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. Pipeline Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will read in our training data. Our training data contains historical flight data from 2009-2018 and contains the following data:\n",
    "- `YEAR`: The year of the flight\n",
    "- `QUARTER`: The yearly quarter\n",
    "- `MONTH`: The month of the flight\n",
    "- `DAYOFMONTH`: The day of the flight\n",
    "- `DAYOFWEEK`: The day of the week the flight took place\n",
    "- `UNIQUECARRIER`: The abbreviation code of the airline\n",
    "- `ORIGIN`: The airport code of the departure airport\n",
    "- `DEST`: The airport code of the arrival airport\n",
    "- `CRSDEPTIME`: The scheduled departure time of the flight in the reservation system\n",
    "- `DEPTIME`: The actual departure time of the flight\n",
    "- `DEPDELAY`: The difference between the actual and scheduled departure times (minutes)\n",
    "- `DEPDEL15`: Departure delay > 15 mins?\n",
    "- `TAXIOUT`: Taxi out time (minutes)\n",
    "- `WHEELSOFF`: Wheels of time (local time)\n",
    "- `CRSARRTIME`: The scheduled arrival time of the flight in the reservation system\n",
    "- `ARRTIME`: The actual arrival time of the flight\n",
    "- `ARRDELAY`: The difference between the actual and scheduled arrival times (minutes)\n",
    "- `CRSELAPSEDTIME`: The scheduled elapsed time of the flight in the reservation system\n",
    "- `AIRTIME`: Flight time (minutes)\n",
    "- `DISTANCEGROUP`: 250 mile distance interval group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](media/ontime.png)\n",
    "\n",
    "**Figure 1:** This figure shows the stages of a flight as described in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = %sql SELECT * FROM ONTIME.TRAIN\n",
    "df = pd.DataFrame(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The dataset has',df.shape[0],'rows and',df.shape[1],'columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at some summary statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Target variable\n",
    "We will use the `ARRDELAY` variable to create our target variable `FLIGHTSTATUS`:\n",
    "- 0 = on time or early\n",
    "- 1 = delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first check for any missing values in this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of missing values\n",
    "df['ARRDELAY'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets investigate why these values are NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary dataframe that includes rows where \"ARRDELAY\" is missing\n",
    "tmp = df[df['ARRDELAY'].isna()]\n",
    "tmp[['CRSARRTIME','ARRTIME','ARRDELAY']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that these flights infact arrived on time: `CRSARRTIME` = `ARRTIME`. So we can replace these missing values in `ARRDELAY` with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing values with 0\n",
    "df['ARRDELAY']=df['ARRDELAY'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to create our target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our target variable:\n",
    "# 0 = on time or early\n",
    "# 1 = delayed\n",
    "target = 'FLIGHTSTATUS'\n",
    "status = []\n",
    "for val in df['ARRDELAY']:\n",
    "    if val <=0:\n",
    "        status.append(0)\n",
    "    else:\n",
    "        status.append(1) \n",
    "\n",
    "df[target] = status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the distribution of our target variable\n",
    "df[target].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see our dataset is slightly imbalanced with 62% of records being flights that are on time or early, and 38% of flights being delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop unneeded columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drop the following columns:\n",
    "- `ARRDELAY`: Will use the target variable instead\n",
    "- `ARRTIME`: Will use the CRSArrTime instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that are no longer needed\n",
    "cols_to_drop=['ARRDELAY','ARRTIME']\n",
    "df=df.drop(cols_to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The dataset now has',df.shape[0],'rows and',df.shape[1],'columns.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue with further data preparation steps, we will first split our dataset into a training and test set using an 80/20 split. We will use our training data to develop our model and our test set to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X = df.drop(target,axis=1)\n",
    "y = df[target]\n",
    "\n",
    "# # 80/20 train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The training dataset has',X_train.shape[0],'rows and',X_train.shape[1],'columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The test dataset has',X_test.shape[0],'rows and',X_test.shape[1],'columns.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "\n",
    "Most machine learning algorithms cannot work with missing values. Therefore, it is important to identify any missing values and then to impute (i.e., replace) any missing values in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage of missing values and show top 10 columns with most missing values\n",
    "percent_missing = X_train.isna().sum() * 100 / len(X_train)\n",
    "percent_missing.sort_values(ascending=False, inplace=True)\n",
    "percent_missing[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are missing values, so imputation is required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DepDelay\n",
    "Let's do some missing values investigation with `DEPDELAY` and `DEPDEL15`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of missing values\n",
    "print('Number of missing values in DEPDELAY column: ',df['DEPDELAY'].isna().sum())\n",
    "print('Number of missing values in DEPDEL15 column: ',df['DEPDEL15'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary dataframe that includes rows where \"DEPDELAY\" is missing\n",
    "tmp = df[df['DEPDELAY'].isna()]\n",
    "tmp[['CRSDEPTIME','DEPTIME','DEPDELAY','DEPDEL15']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that these flights infact departed on time: `CRSDEPTIME` = `DEPTIME`. So we can replace these missing values in `DEPDELAY` and `DEPDEL15` with 0. \n",
    "\n",
    "We will also drop the feature `DEPTIME` from our data set as it is highly correlated to `DEPDELAY` (i.e. we can use the values in `CRSDEPTIME` and `DELDELAY` to obtain the values in `DEPTIME`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop DEPTIME from our training and test sets\n",
    "X_train=X_train.drop(['DEPTIME'],axis=1)\n",
    "X_test=X_test.drop(['DEPTIME'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform our data transformations by wrapping them in the scikit-learn Pipeline construct - this will allow us to chain our other data transformations together later on and combine them with our ML model to simplify the code required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](media/pipeline.png)\n",
    "\n",
    "**Figure 2:** This image shows how we are using scikit-learn's Pipeline and ColumnTransformer constructs to build a machine learning pipeline that performs data transformations in parallel, concatenates the results, and feeds the transformed data to the ML model. Each of the dashed black boxes represents a sub-pipeline that handles a particular transformation on a subset of our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first create a pipeline for the missing values imputer - note that we have not included `DEPDELAY` in this pipeline. This is because we also want to perform feature scaling on it, so we will impute that feature in a following step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for missing values imputation\n",
    "features_to_impute = ['DEPDEL15']\n",
    "missing_vals_imputer = Pipeline( steps = [\n",
    "    ( 'imputer', SimpleImputer(strategy='constant',fill_value=0) )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Time Values\n",
    "We will convert the timestamp values in `CRSDepTime`,`WheelsOff`, and `CRSArrTime` to a categorical value split into 4 quarters of the day. We will split `CRSElapsedTime` into 30 min segments. To incorporate this into a Pipeline construct, we first need to create a custom class that performs this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom transformer that breaks time values into categorical values\n",
    "class TimeTransformer( BaseEstimator, TransformerMixin ):\n",
    "    #Class constructor method - takes no inputs, so nothing to do\n",
    "    def __init__(self):\n",
    "        return None\n",
    "        \n",
    "    #Return self nothing else to do here\n",
    "    def fit( self, X, y = None  ):\n",
    "        return self\n",
    "    \n",
    "    #Transformer method we wrote for this transformer \n",
    "    # Converts 24hr features into 4 quarters of the day, splits elapsed time into 30min segments\n",
    "    def transform(self, X , y = None ):\n",
    "        X['CRSDEPTIME'] = np.ceil(X['CRSDEPTIME']/600).apply(int)\n",
    "        X['WHEELSOFF'] = np.ceil(X['WHEELSOFF']/600).apply(int) \n",
    "        X['CRSARRTIME'] = np.ceil(X['CRSARRTIME']/600).apply(int)\n",
    "        X['CRSELAPSEDTIME']=np.ceil(X['CRSELAPSEDTIME']/30).apply(int)\n",
    "        \n",
    "        return X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that converts our time values using the custom transformer we just wrote\n",
    "time_features = ['CRSDEPTIME','WHEELSOFF','CRSARRTIME','CRSELAPSEDTIME']\n",
    "time_vals_converter = Pipeline( steps = [\n",
    "    ( 'time_transformer', TimeTransformer())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "We will scale our continuous numerical columns: `AirTime`,`TaxiOut`,`DepDelay`.\n",
    "\n",
    "Most machine learning algorithms do not perform well when the numerical attributes in the data have very different scales (i.e., large range of possible values). It is important to make sure that the numerical features in our dataset have similar scales. A standardized value z of an input feature x is calculated as:\n",
    "\n",
    "z = (x - u) / s\n",
    "\n",
    "where u is the mean of the training samples and s is the standard deviation of the training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale continuous numerical columns\n",
    "features_to_scale=['AIRTIME','TAXIOUT','DEPDELAY']\n",
    "feature_scaler = Pipeline( steps = [\n",
    "    ( 'imputer', SimpleImputer(strategy='constant',fill_value=0)), # Imputation - DEPDELAY has missing values\n",
    "    ( 'scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values in `UNIQUECARRIER`, `ORIGIN`, and `DEST` need to be converted into categorical columns\n",
    "\n",
    "Most machine learning algorithms prefer to work with numbers rather than text values. A common solution is to create one binary attribute *per category*: one attribute equal to 1 when the category is “LAX” (and 0 otherwise), another attribute equal to 1 when the category is “JFK” (and 0 otherwise), and so on. This is called one-hot encoding, because only one attribute will be equal to 1 (hot), while the others will be 0 (cold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of unique values in each of these columns\n",
    "print('Number of unique values in UNIQUECARRIER:', X_train['UNIQUECARRIER'].nunique())\n",
    "print('Number of unique values in ORIGIN:', X_train['ORIGIN'].nunique())\n",
    "print('Number of unique values in DEST:', X_train['DEST'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, we can't apply one-hot encoding directly to these columns as we would be adding >750 new columns! Instead we will select the top 10 most common occurences in these columns as our labels. Again to do this, we need to create a custom class that will first identify the top 10 most common occurences, and then will transform the values not in those top 10 lists to \"Other_X\" (e.g. Other_Airline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom transformer that reduces the number of unique values\n",
    "class ReduceUnique( BaseEstimator, TransformerMixin ):\n",
    "    #Class constructor method - takes no inputs, so nothing to do\n",
    "    def __init__(self):\n",
    "        return None\n",
    "        \n",
    "    #Fit method - find the top 10 most common occurrences in each of the 3 columns\n",
    "    def fit( self, X, y = None  ):\n",
    "        self._top_10_airlines = X_train['UNIQUECARRIER'].value_counts().index[:10].tolist()\n",
    "        self._top_10_origin = X_train['ORIGIN'].value_counts().index[:10].tolist()\n",
    "        self._top_10_dest = X_train['DEST'].value_counts().index[:10].tolist()\n",
    "        return self\n",
    "    \n",
    "    #Transformer method - convert values not in the top 10 lists to \"Other_X\"\n",
    "    def transform(self, X , y = None ):\n",
    "        X['UNIQUECARRIER'] = np.where(~X['UNIQUECARRIER'].isin(self._top_10_airlines),\n",
    "                                      'Other_Airline', X['UNIQUECARRIER'])\n",
    "        X['ORIGIN'] = np.where(~X['ORIGIN'].isin(self._top_10_origin),'Other_Origin', X['ORIGIN'])\n",
    "        X['DEST'] = np.where(~X['DEST'].isin(self._top_10_dest),'Other_Dest', X['DEST'])\n",
    "        \n",
    "        return X.values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having applied this `ReduceUnique` transformer to our columns, we can now perform one-hot encoding as we now only replace the 3 original columns with 11+11+11=33 new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding\n",
    "features_to_encode=['UNIQUECARRIER','ORIGIN','DEST']\n",
    "feature_encoder = Pipeline( steps = [\n",
    "    ( 'reduceunique', ReduceUnique()), # First reduce the number of unique values\n",
    "    ('encoder',OneHotEncoder(handle_unknown='ignore')) # Perform one-hot encoding\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine our transformers into a single data transformation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ColumnTransformer construct allows us to perform our transformations *in parallel* improving the overall execution time of our pipeline. It does so by applying different transformers to different subsets of the whole input data and concatenating the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_pipeline=ColumnTransformer([\n",
    "    ('imputer',missing_vals_imputer,features_to_impute), # Missing values imputation\n",
    "    ('time_vals_converter',time_vals_converter,time_features), # 24hr time values conversion\n",
    "    ('feature_scaler', feature_scaler,features_to_scale ), # Feature scaling\n",
    "    ('feature_encoder', feature_encoder,features_to_encode ) # One-hot encoding\n",
    "],remainder='passthrough',verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](media/pipeline.png)\n",
    "\n",
    "**Figure 2:** This image shows how we are using scikit-learn's Pipeline and ColumnTransformer constructs to build a machine learning pipeline that performs data transformations in parallel, concatenates the results, and feeds the transformed data to the ML model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Logistic Regression (also called Logit Regression) is commonly used to estimate the probability that an instance belongs to a particular class (e.g., what is the probability that you will pass your exam?). If the estimated probability is greater than 50%, then the model predicts that the instance belongs to that class (called the positive class, labeled “1”), and otherwise it predicts that it does not (i.e., it belongs to the negative class, labeled “0”). This makes it a binary classifier.\n",
    "\n",
    "We will train a logistic regression model using the default values and setting the `class_weight` parameter to \"balanced\" due to our imbalanced training dataset.\n",
    "\n",
    "![](media/Exam_pass_logistic_curve.jpeg)\n",
    "\n",
    "**Figure 3:** A sample logistic regression model. The blue line shows the fitted logit function, and the black dots represent the input training data. Note that for points where the probability is greater than 50%, the predicted class is 1, and 0 otherwise. This model predicts that if you study for 2.75 hrs or longer, you will pass your exam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our final ML pipeline by adding our model to the data transformation pipeline\n",
    "full_pipeline = Pipeline( steps = [\n",
    "    ('transformation_pipeline', transform_pipeline), # Data transformations\n",
    "    ( 'model', LogisticRegression(random_state=42,class_weight='balanced')) # Logistic Regression model\n",
    "],verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Perform data transformations and fit our model to the transformed training data\n",
    "trained_pipeline = full_pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions on our test set to compare it against the true values\n",
    "y_pred = trained_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the F1 score (harmonic mean of Precision and Recall) as our metric for evaluating model performance. An F1-Score of 1.0, indicates perfect precision and recall, and the lowest possible value is 0, if either the precision or the recall is zero.\n",
    "\n",
    "Precision is the number of true positive results divided by the number of all positive results, including those not identified correctly, and the recall is the number of true positive results divided by the number of all samples that should have been identified as positive.\n",
    "\n",
    "The F1 score can be expressed as\n",
    "\n",
    "$\\frac{TP}{TP +\\frac{1}{2}(FP+FN)}$,\n",
    "\n",
    "where TP is the number of true positives, FP is the number of false positives, and FN is the number of false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's evaluate the model performance\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "print('Model F1 Score: %.3f' % f1_score(y_test,y_pred))\n",
    "\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n",
    "            fmt='.2%', cmap='Blues',xticklabels=['On Time','Delayed'],yticklabels=['On Time','Delayed'])\n",
    "plt.title('Random Forest')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Deployment Assets\n",
    "\n",
    "We now need to save our deployment assets to Db2. They include:\n",
    "- Our trained ML pipeline, saved as a joblib file and stored in a Db2 table\n",
    "- Our custom transformers, saved as a Python script and stored on the shared filesystem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save our ML pipeline to a Db2 table\n",
    "\n",
    "1. Save our asset as a joblib file\n",
    "2. Create a table MYASSETS for storing the contents of the joblib file\n",
    "3. Use the ibm_db package to store the contents of the joblib file to the MYASSETS table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our asset as a joblib file\n",
    "dump(trained_pipeline,'../../../PYUDF/trained_pipeline.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS MYASSETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE TABLE MYASSETS(name VARCHAR(15),joblib BLOB) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function for saving the contents of a joblib file to a Db2 table\n",
    "def deploy_joblib_assets(asset_name,filepath,table_name,ibm_db_conn):\n",
    "    '''\n",
    "    asset_name = name of the asset - to be stored in Db2 table column\n",
    "    filepath = the path to the joblib file to be stored\n",
    "    table_name = the Db2 table which will store the joblib file contents and the asset name\n",
    "    ibm_db_conn = an connection to the Db2 database created by the ibm_db.connect() function\n",
    "    '''\n",
    "    \n",
    "    joblib_file=filepath\n",
    "    name=asset_name\n",
    "\n",
    "    print(\"Preparing to deploy asset \"+asset_name+\" to Db2...\")\n",
    "    \n",
    "    # Prepare SQL INSERT statement\n",
    "    insert_sql = \"INSERT INTO \"+table_name+\"(name,joblib) values(?,?)\"\n",
    "    stmt = ibm_db.prepare(ibm_db_conn, insert_sql)\n",
    "    print(\"Successfully prepared the insert statement\")\n",
    "    \n",
    "    # Bind parameters (name & joblib file contents) to the placeholders\n",
    "    rc = ibm_db.bind_param(stmt, 1, name)\n",
    "    print(\"Bind returned: \"+str(rc))\n",
    "    rc = ibm_db.bind_param(stmt, 2, joblib_file, ibm_db.PARAM_FILE,ibm_db.SQL_BLOB )\n",
    "    print(\"Bind returned: \"+str(rc))\n",
    "    print(\"Successfully bound the variables to the parmameter-markers\")\n",
    "    \n",
    "    # Execute the INSERT statement\n",
    "    try:\n",
    "        ibm_db.execute(stmt)\n",
    "        print(\"Successfully inserted joblib file into blob column\\n\")\n",
    "    except:\n",
    "        print(\"Failed to execute the insert to blob column\")\n",
    "        print(ibm_db.stmt_errormsg(stmt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a connection to the Db2 database - for use in our custom function\n",
    "conn_str = \"DATABASE=ONTIME;\" + \\\n",
    "           \"HOSTNAME=localhost;\"+ \\\n",
    "           \"PROTOCOL=TCPIP;\"  + \\\n",
    "           \"PORT=50001;\" + \\\n",
    "           \"UID=db2inst1;\" + \\\n",
    "           \"PWD=db2inst1;\"\n",
    "\n",
    "ibm_db_conn = ibm_db.connect(conn_str,\"\",\"\")\n",
    "print('Connection to Db2 Instance Created!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our asset to the MYASSETS table\n",
    "deploy_joblib_assets(\"pipeline\",\"../../../PYUDF/trained_pipeline.joblib\",\"MYASSETS\",ibm_db_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving our custom functions\n",
    "\n",
    "We will simply create a python script `mycustomtransformers.py` that we will use to load in our custom transformers into our UDF in a later step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../../../PYUDF/mycustomtransformers.py\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "#Custom transformer that breaks time values into categorical values\n",
    "class TimeTransformer( BaseEstimator, TransformerMixin ):\n",
    "    #Class constructor method - takes no inputs, so nothing to do\n",
    "    def __init__(self):\n",
    "        return None\n",
    "        \n",
    "    #Return self nothing else to do here\n",
    "    def fit( self, X, y = None  ):\n",
    "        return self\n",
    "    \n",
    "    #Transformer method we wrote for this transformer \n",
    "    # Converts 24hr features into 4 quarters of the day, splits elapsed time into 30min segments\n",
    "    def transform(self, X , y = None ):\n",
    "        X['CRSDEPTIME'] = np.ceil(X['CRSDEPTIME']/600).apply(int)\n",
    "        X['WHEELSOFF'] = np.ceil(X['WHEELSOFF']/600).apply(int) \n",
    "        X['CRSARRTIME'] = np.ceil(X['CRSARRTIME']/600).apply(int)\n",
    "        X['CRSELAPSEDTIME']=np.ceil(X['CRSELAPSEDTIME']/30).apply(int)\n",
    "        \n",
    "        return X.values \n",
    "\n",
    "\n",
    "#Custom transformer that reduces the number of unique values\n",
    "class ReduceUnique( BaseEstimator, TransformerMixin ):\n",
    "    #Class constructor method - takes no inputs, so nothing to do\n",
    "    def __init__(self):\n",
    "        return None\n",
    "        \n",
    "    #Fit method - find the top 10 most common occurrences in each of the 3 columns\n",
    "    def fit( self, X, y = None  ):\n",
    "        self._top_10_airlines = X_train['UNIQUECARRIER'].value_counts().index[:10].tolist()\n",
    "        self._top_10_origin = X_train['ORIGIN'].value_counts().index[:10].tolist()\n",
    "        self._top_10_dest = X_train['DEST'].value_counts().index[:10].tolist()\n",
    "        return self\n",
    "    \n",
    "    #Transformer method - convert values not in the top 10 lists to \"Other_X\"\n",
    "    def transform(self, X , y = None ):\n",
    "        X['UNIQUECARRIER'] = np.where(~X['UNIQUECARRIER'].isin(self._top_10_airlines),\n",
    "                                      'Other_Airline', X['UNIQUECARRIER'])\n",
    "        X['ORIGIN'] = np.where(~X['ORIGIN'].isin(self._top_10_origin),'Other_Origin', X['ORIGIN'])\n",
    "        X['DEST'] = np.where(~X['DEST'].isin(self._top_10_dest),'Other_Dest', X['DEST'])\n",
    "        \n",
    "        return X.values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Deploying the Pipeline to Db2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to deploy our model to Db2 for in-database scoring.\n",
    "\n",
    "We will:\n",
    "1. Create a UDF source file\n",
    "2. Register our Python UDF\n",
    "3. Call our UDF with a SQL statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our UDF (User Defined Function) has the following sections:\n",
    "- **Initialization**: Define any static variables, load our deployment assets (e.g. model), and prepare for batch scoring\n",
    "- **Data Preparation**: Perform the same data preparation steps we performed during model development\n",
    "- **Model Scoring & Output**: Call our model to make predictions and return the results\n",
    "\n",
    "Our UDF will output the following:\n",
    "- `DATE`: The date of the flight \n",
    "- `ORIGIN`: The origin airport\n",
    "- `DEST`: The destination airport\n",
    "- `CARRIER`: The airline code\n",
    "- `CRSDEPTIME`: The scheduled departure time\n",
    "- `CRSARRTIME`: The scheduled arrival time \n",
    "- `PREDICTION`: The prediction (0 = on time, 1 = delayed)\n",
    "- `PROB_DELAYED`: The predicted probability that the flight will be delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](media/udfdeployment.png)\n",
    "\n",
    "**Figure 4**: This figure depicts how a UDF source file is created from the original pipeline code developed during the model development stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../../../PYUDF/myUDF.py\n",
    "\n",
    "#Imports\n",
    "import nzae\n",
    "import pandas as pd\n",
    "from joblib import load\n",
    "import numpy as np\n",
    "import ibm_db\n",
    "from io import BytesIO\n",
    "sys.path.insert(0, 'home/db2inst1/PYUDF/')\n",
    "from mycustomtransformers import TimeTransformer, ReduceUnique\n",
    "\n",
    "class full_pipeline(nzae.Ae):\n",
    "    def _runUdtf(self):\n",
    "        #####################\n",
    "        ### INITIALIZATON ###\n",
    "        #####################\n",
    "        \n",
    "        # Define static variables\n",
    "        input_cols = ['YEAR','QUARTER', 'MONTH',\n",
    "                      'DAYOFMONTH', 'DAYOFWEEK','UNIQUECARRIER',\n",
    "                      'ORIGIN', 'DEST', 'CRSDEPTIME',\n",
    "                      'DEPDELAY', 'DEPDEL15','TAXIOUT','WHEELSOFF',\n",
    "                      'CRSARRTIME', 'CRSELAPSEDTIME', 'AIRTIME', 'DISTANCEGROUP']\n",
    "\n",
    "        # Make a connection to our Db2 database - for executing SQL statements\n",
    "        conn_str = \"DATABASE=ONTIME;\" + \\\n",
    "           \"HOSTNAME=localhost;\"+ \\\n",
    "           \"PROTOCOL=TCPIP;\"  + \\\n",
    "           \"PORT=50001;\" + \\\n",
    "           \"UID=db2inst1;\" + \\\n",
    "           \"PWD=db2inst1;\"\n",
    "\n",
    "        ibm_db_conn = ibm_db.connect(conn_str,\"\",\"\")\n",
    "        \n",
    "        # Load our trained ML pipeline\n",
    "        sql = \"SELECT JOBLIB FROM MYASSETS WHERE NAME='pipeline'\"\n",
    "        stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "        row = ibm_db.fetch_tuple(stmt)\n",
    "        trained_pipeline = load(BytesIO(row[0]))\n",
    "        \n",
    "        #######################\n",
    "        ### DATA COLLECTION ###\n",
    "        #######################\n",
    "        # Collect rows into a single batch\n",
    "        batchsize = 10000\n",
    "        rownum = 0\n",
    "        row_list = []\n",
    "        for row in self:\n",
    "            row_list.append(row)\n",
    "            rownum = rownum+1         \n",
    "\n",
    "            if rownum==batchsize:\n",
    "\n",
    "                # Create a dataframe from the batch\n",
    "                df = pd.DataFrame(row_list, columns=input_cols)\n",
    "\n",
    "                # Save the original datestring (yyyymmdd), ORIGIN, DEST, UNIQUECARRIER, CRSDEPTIME, CRSARRTIME\n",
    "                # to return with the model prediction\n",
    "                dates = [int(year + month + day) for year, month, day in \n",
    "                         zip(map(str,list(df['YEAR'])), map(str,[\"%02d\" %i for i in list(df['MONTH'])]), \n",
    "                             map(str,[\"%02d\" %i for i in list(df['DAYOFMONTH'])]))]\n",
    "                origins = list(df['ORIGIN'])\n",
    "                dests = list(df['DEST'])\n",
    "                carriers = list(df['UNIQUECARRIER'])\n",
    "                deptimes = list(df['CRSDEPTIME'])\n",
    "                arrtimes = list(df['CRSARRTIME'])\n",
    "\n",
    "                ##############################\n",
    "                ### MODEL SCORING & OUTPUT ###\n",
    "                ##############################\n",
    "                \n",
    "                # Call our trained pipeline to transform the data and make predictions\n",
    "                predictions = trained_pipeline.predict(df)\n",
    "                \n",
    "                # Calculate probability that flight will be delayed, round to 2 decimal places              \n",
    "                probability_delayed = [round(x,2) for x in trained_pipeline.predict_proba(df)[:,1]]\n",
    "\n",
    "                # Return the result\n",
    "                for x in range(predictions.shape[0]):\n",
    "                    self.output(int(dates[x]),str(origins[x]),str(dests[x]),str(carriers[x]),\n",
    "                                int(deptimes[x]),int(arrtimes[x]),int(predictions[x]),\n",
    "                                float(probability_delayed[x]))\n",
    "\n",
    "                row_list = []\n",
    "                rownum = 0\n",
    "                \n",
    "        self.done()\n",
    "full_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to register our UDF. We will provide it a name `FLIGHT_PREDICTER`, define the input datatypes, the definition of the table it outputs (i.e. column names and datatypes), and the path to our source file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE OR REPLACE FUNCTION \n",
    "FLIGHT_PREDICTER(SMALLINT,SMALLINT,SMALLINT,SMALLINT,SMALLINT,VARCHAR(8),VARCHAR(3),VARCHAR(3),SMALLINT,\n",
    "                 SMALLINT,SMALLINT,SMALLINT,SMALLINT,SMALLINT,SMALLINT,SMALLINT,SMALLINT) \n",
    "RETURNS TABLE (DATE INTEGER,ORIGIN VARCHAR(3), DEST VARCHAR(3), CARRIER VARCHAR(3), \n",
    "CRSDEPTIME SMALLINT, CRSARRTIME SMALLINT,PREDICTION SMALLINT,PROB_DELAYED DOUBLE)\n",
    "LANGUAGE PYTHON PARAMETER STYLE NPSGENERIC  FENCED  NOT THREADSAFE  NO FINAL CALL  DISALLOW PARALLEL  NO DBINFO  \n",
    "DETERMINISTIC NO EXTERNAL ACTION CALLED ON NULL INPUT  \n",
    "NO SQL EXTERNAL NAME 'home/db2inst1/PYUDF/myUDF.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our UDF source file created and our function registered, we can now call the Python UDF to make predictions. We will create a table `ONTIME.PREDICTIONS` to store the results, and insert the output from our UDF into that table. Recall that we will be making predictions on new flight data from 2020 (i.e., the model is using historical data to make predictions on future events)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS ONTIME.PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE TABLE ONTIME.PREDICTIONS(DATE INTEGER,\n",
    "                                ORIGIN VARCHAR(3), \n",
    "                                DEST VARCHAR(3),\n",
    "                                CARRIER VARCHAR(3),\n",
    "                                CRSDEPTIME SMALLINT, \n",
    "                                CRSARRTIME SMALLINT,\n",
    "                                PREDICTION SMALLINT,\n",
    "                                PROB_DELAYED DOUBLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "INSERT INTO ONTIME.PREDICTIONS(DATE,ORIGIN,DEST,CARRIER,CRSDEPTIME,CRSARRTIME,PREDICTION,PROB_DELAYED)\n",
    "SELECT f.* from ONTIME.NEWDATA i,\n",
    "TABLE(FLIGHT_PREDICTER(i.YEAR, i.QUARTER, i.MONTH, \n",
    "                i.DAYOFMONTH, i.DAYOFWEEK,i.UNIQUECARRIER,\n",
    "                i.ORIGIN, i.DEST, i.CRSDEPTIME,\n",
    "                i.DEPDELAY, i.DEPDEL15,i.TAXIOUT,i.WHEELSOFF,\n",
    "                i.CRSARRTIME, i.CRSELAPSEDTIME, i.AIRTIME, i.DISTANCEGROUP)) f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can query the `ONTIME.PREDICTIONS` table to look at the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = %sql SELECT * FROM ONTIME.PREDICTIONS\n",
    "result = pd.DataFrame(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Lets look at the first 10 rows of our UDF's output\n",
    "result.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Build and deploy your own UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will deploy your own ML classification model. We will provide the steps to build and deploy the model, including any necessary data transformation steps. You will be responsible for filling the UDF template provided during the model deployment phase.\n",
    "\n",
    "The dataset used is the famous IRIS dataset - it contains samples from each of three species of *Iris* flowers. (*Iris setosa*, *Iris virginica*, and *Iris versicolor*). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "import numpy as np\n",
    "\n",
    "# Model building\n",
    "from sklearn.model_selection import train_test_split # Train-test split\n",
    "from sklearn.impute import SimpleImputer # Missing values imputation\n",
    "from sklearn.compose import ColumnTransformer # For applying transformation objects\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score # Model evaluation\n",
    "from joblib import dump #Saving deployment assets\n",
    "\n",
    "# Plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(rc={'figure.figsize':(12,8)})\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load the Db2 magic functions and make a connection to our Db2 database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%run db2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run connectiondb2ml.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the training data\n",
    "query = %sql SELECT * FROM ONTIME.IRIS_TRAIN\n",
    "df = pd.DataFrame(query)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First drop the ID column as it is only a row identifier\n",
    "df=df.drop(\"ID\",axis=1)\n",
    "\n",
    "# Train test split\n",
    "target = \"CLASS\"\n",
    "X = df.drop(target,axis=1)\n",
    "y = df[target]\n",
    "\n",
    "# Our dataset size is small, so we will do a 70/30 train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The training dataset has',X_train.shape[0],'rows and',X_train.shape[1],'columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The test dataset has',X_test.shape[0],'rows and',X_test.shape[1],'columns.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check for missing values in our training and test sets, and perform imputation if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of missing values in the training set\n",
    "number_of_missing_vals = X_train.isna().sum()\n",
    "number_of_missing_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's perform missing value imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create pipeline\n",
    "pipeline = Pipeline([('imputer',SimpleImputer(strategy='mean')), #Impute missing values\n",
    "                    ('model',DecisionTreeClassifier(random_state=42))]) #Decision Tree model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's repeat these steps for our test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation\n",
    "\n",
    "We will train a simple decision tree model to predict the type of flower based on the sepal and petal length and width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data and fit our ML model\n",
    "fitted_pipe=pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set to evaluate the model performance\n",
    "y_pred = fitted_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next will make predictions on our test set and evaluate the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "print('Model F1 Score: %.3f' % f1_score(y_test,y_pred,average='weighted'))\n",
    "\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n",
    "            fmt='.2%', cmap='Blues',xticklabels=['Iris-setosa','Iris-virginica','Iris-versicolor'],\n",
    "            yticklabels=['Iris-setosa','Iris-virginica','Iris-versicolor'])\n",
    "plt.title('Decision Tree')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Deployment Assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are satisfied with our model, we will save the trained imputer and trained model to the shared filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our model and scalar\n",
    "dump(fitted_pipe,\"../../../PYUDF/exercise_pipeline.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS MYASSETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE TABLE MYASSETS(name VARCHAR(15),joblib BLOB) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_joblib_assets(\"mypipeline\",\"../../../PYUDF/exercise_pipeline.joblib\",\"MYASSETS\",ibm_db_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Deployment\n",
    "\n",
    "Here, we have provided a UDF template for you to fill out.\n",
    "\n",
    "1. Write a SQL statment to load in the pipeline from the MYASSETS table (line 28)\n",
    "2. Add some code to make predictions with the ML pipeline (line 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../../../PYUDF/exerciseUDF.py\n",
    "\n",
    "#Imports\n",
    "import nzae\n",
    "import pandas as pd\n",
    "from joblib import load\n",
    "import numpy as np\n",
    "import ibm_db\n",
    "from io import BytesIO\n",
    "\n",
    "class full_pipeline(nzae.Ae):\n",
    "    def _runUdtf(self):\n",
    "        #####################\n",
    "        ### INITIALIZATON ###\n",
    "        #####################\n",
    "        \n",
    "        # Load deployment assets from Db2 tables\n",
    "        conn_str = \"DATABASE=ONTIME;\" + \\\n",
    "           \"HOSTNAME=localhost;\"+ \\\n",
    "           \"PROTOCOL=TCPIP;\"  + \\\n",
    "           \"PORT=50001;\" + \\\n",
    "           \"UID=db2inst1;\" + \\\n",
    "           \"PWD=db2inst1;\"\n",
    "\n",
    "        ibm_db_conn = ibm_db.connect(conn_str,\"\",\"\")\n",
    "        \n",
    "        # Load ML pipeline\n",
    "        sql = ## YOUR SQL STATEMENT HERE ##\n",
    "        stmt = ibm_db.exec_immediate(ibm_db_conn, sql)\n",
    "        row = ibm_db.fetch_tuple(stmt)\n",
    "        pipeline = load(BytesIO(row[0]))\n",
    "\n",
    "        # Define static variable\n",
    "        input_cols=['SEPAL_LENGTH','SEPAL_WIDTH','PETAL_LENGTH','PETAL_WIDTH']\n",
    "     \n",
    "        # Collect rows into a single batch\n",
    "        batchsize = 110\n",
    "        rownum = 0\n",
    "        row_list = []\n",
    "        for row in self:\n",
    "            row_list.append(row)\n",
    "            rownum = rownum+1         \n",
    "\n",
    "            if rownum==batchsize:\n",
    "                ########################\n",
    "                ### DATA PREPARATION ###\n",
    "                ########################\n",
    "                \n",
    "                # Separate the IDs out for returning with the predictions\n",
    "                ids=np.array(row_list)[:,0]\n",
    "                \n",
    "                # Collect the rows into a dataframe\n",
    "                df = pd.DataFrame(np.array(row_list)[:,1:5], columns=input_cols)\n",
    "\n",
    "                ##############################\n",
    "                ### MODEL SCORING & OUTPUT ###\n",
    "                ##############################\n",
    "                \n",
    "                # Call model to make prediction\n",
    "                predictions = ## YOUR CODE HERE ##\n",
    "                \n",
    "                # Return the result\n",
    "                for x in range(predictions.shape[0]):\n",
    "                    self.output(int(ids[x]),str(predictions[x]))\n",
    "\n",
    "                row_list = []\n",
    "                rownum = 0      \n",
    "        self.done()\n",
    "full_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the following cells to check your work. **Hint:** If you encounter errors, the UDF log files can be found in the `sqllib/db2dump/pythonUDX/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE OR REPLACE FUNCTION \n",
    "MY_UDF(INT,DOUBLE,DOUBLE,DOUBLE,DOUBLE) \n",
    "RETURNS TABLE (ID INT,PREDICTION VARCHAR(15))\n",
    "LANGUAGE PYTHON PARAMETER STYLE NPSGENERIC  FENCED  NOT THREADSAFE  NO FINAL CALL  DISALLOW PARALLEL  NO DBINFO  \n",
    "DETERMINISTIC NO EXTERNAL ACTION CALLED ON NULL INPUT  \n",
    "NO SQL EXTERNAL NAME 'home/db2inst1/PYUDF/exerciseUDF.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS ONTIME.IRIS_PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE TABLE ONTIME.IRIS_PREDICTIONS(ID INT,PREDICTION VARCHAR(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%sql\n",
    "INSERT INTO ONTIME.IRIS_PREDICTIONS(ID,PREDICTION)\n",
    "SELECT f.* from ONTIME.IRIS_TEST i,\n",
    "TABLE(MY_UDF(i.ID,i.SEPAL_LENGTH,i.SEPAL_WIDTH,i.PETAL_LENGTH,i.PETAL_WIDTH)) f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = %sql SELECT * FROM ONTIME.IRIS_PREDICTIONS\n",
    "result = pd.DataFrame(query)\n",
    "result.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Video: Deploying a Machine Learning Model Trained on IBM Cloud Pak for Data into Db2](https://video.ibm.com/recorded/129516812)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
