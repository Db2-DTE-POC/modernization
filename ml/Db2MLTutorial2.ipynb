{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"media/Assets&ArchHeader.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Operationalize a Scikit-learn Model with Db2 v11.5\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional scoring of machine learning models with data stored in a database often involves transferring the scoring data to a deployment system where the trained model is stored.  This can cause major challenges such as data transfer costs or data security requirements that can limit the kind or amount of data transferred from the database, such as personal identifiable information. Additionally, hardware and software roadblocks can ultimately prevent the transfer of large amounts of scoring data.\n",
    "\n",
    "In this workshop we will develop a machine learning pipeline externally (i.e. on a local Jupyter notebook) and deploy it to Db2 for in-database scoring via SQL commands.\n",
    "\n",
    "This solution allows users such as business analysts using SQL-based tools and applications to execute the machine learning pipeline by simply querying the Db2 database. \n",
    "\n",
    "This process ultimately keeps your scoring data secure and reduces data transfer latency, required compute power, complexity, and cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisites**: If you are unfamiliar with the Jupyter Notebook environment, it is recommended to have gone through the [An Introduction to Jupyter Notebooks](./An_Introduction_to_Jupyter_Notebooks.ipynb) lab first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider we are on a plane heading towards our destination. We want to determine if the flight will be delayed arriving or not after it leaves the origin airport (binary classification):\n",
    "* A delayed flight will be a flight that arrives late at its destination\n",
    "* If the flight has any delays from its departure, but still arrives to its destination on time, it will not be considered a delayed flight\n",
    "* A *canceled* flight is not a delayed flight as it never left nor arrived to its destination for whatever reason\n",
    "* A *diverted* flight is not a delayed flight as it was diverted from its destination\n",
    "\n",
    "To solve this problem, we will use historical US flight data from 2009-2018. We will then deploy our model to Db2 and make predictions on \"new\" data from 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Connection to Db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "import numpy as np\n",
    "\n",
    "# Model building\n",
    "from sklearn.model_selection import train_test_split # Train-test split\n",
    "from sklearn.impute import SimpleImputer # Missing values imputation\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler # Feature scaling and one-hot encoding\n",
    "from sklearn.compose import ColumnTransformer # For applying transformation objects\n",
    "from sklearn.ensemble import RandomForestClassifier # Model\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score # Model evaluation\n",
    "from joblib import dump #Saving deployment assets\n",
    "\n",
    "# Plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(rc={'figure.figsize':(12,8)})\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load the Db2 magic functions and make a connection to our Db2 database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run db2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run connectiondb2ml.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql option maxrows -1 display pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. Pipeline Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will read in our training data. Our training data contains historical flight data from 2009-2018 and contains the following data:\n",
    "- `YEAR`: The year of the flight\n",
    "- `QUARTER`: The yearly quarter\n",
    "- `MONTH`: The month of the flight\n",
    "- `DAYOFMONTH`: The day of the flight\n",
    "- `DAYOFWEEK`: The day of the week the flight took place\n",
    "- `UNIQUECARRIER`: The abbreviation code of the airline\n",
    "- `ORIGIN`: The airport code of the departure airport\n",
    "- `DEST`: The airport code of the arrival airport\n",
    "- `CRSDEPTIME`: The scheduled departure time of the flight in the reservation system\n",
    "- `DEPTIME`: The actual departure time of the flight\n",
    "- `DEPDELAY`: The difference between the actual and scheduled departure times (minutes)\n",
    "- `DEPDEL15`: Departure delay > 15 mins?\n",
    "- `TAXIOUT`: Taxi out time (minutes)\n",
    "- `WHEELSOFF`: Wheels of time (local time)\n",
    "- `CRSARRTIME`: The scheduled arrival time of the flight in the reservation system\n",
    "- `ARRTIME`: The actual arrival time of the flight\n",
    "- `ARRDELAY`: The difference between the actual and scheduled arrival times (minutes)\n",
    "- `CRSELAPSEDTIME`: The scheduled elapsed time of the flight in the reservation system\n",
    "- `AIRTIME`: Flight time (minutes)\n",
    "- `DISTANCEGROUP`: 250 mile distance interval group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](media/ontime.png)\n",
    "\n",
    "**Figure 1:** This figure shows the stages of a flight as described in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query = %sql SELECT * FROM ONTIME.TRAIN\n",
    "df = pd.DataFrame(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The dataset has',df.shape[0],'rows and',df.shape[1],'columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at some summary statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Target variable\n",
    "We will use the `ArrDelay` variable to create our target variable `FlightStatus`:\n",
    "- 0 = on time or early\n",
    "- 1 = delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first check for any missing values in this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of missing values\n",
    "df['ARRDELAY'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets investigate why these values are NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary dataframe that includes rows where \"ARRDELAY\" is missing\n",
    "tmp = df[df['ARRDELAY'].isna()]\n",
    "tmp[['CRSARRTIME','ARRTIME','ARRDELAY']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that these flights infact arrived on time: `CRSArrTime` = `ArrTime`. So we can replace these missing values in `ArrDelay` with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing values with 0\n",
    "df['ARRDELAY']=df['ARRDELAY'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to create our target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our target variable:\n",
    "# 0 = on time or early\n",
    "# 1 = delayed\n",
    "target = 'FLIGHTSTATUS'\n",
    "status = []\n",
    "for val in df['ARRDELAY']:\n",
    "    if val <=0:\n",
    "        status.append(0)\n",
    "    else:\n",
    "        status.append(1) \n",
    "\n",
    "df[target] = status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the distribution of our target variable\n",
    "df[target].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see our dataset is slightly imbalanced with 62% of records being flights that are on time or early, and 38% of flights being delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DepDelay\n",
    "Let's do the same missing values investigation with `DepDelay`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of missing values\n",
    "df['DEPDELAY'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary dataframe that includes rows where \"DEPDELAY\" is missing\n",
    "tmp = df[df['DEPDELAY'].isna()]\n",
    "tmp[['CRSDEPTIME','DEPTIME','DEPDELAY','DEPDEL15']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that these flights infact departed on time: `CRSDepTime` = `DepTime`. So we can replace these missing values in `DepDelay` and `DepDel15` with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing values with 0\n",
    "df['DEPDELAY']=df['DEPDELAY'].fillna(0)\n",
    "df['DEPDEL15']=df['DEPDEL15'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop unneeded columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drop the following columns:\n",
    "- `DEPTIME`: Will use the DepDelay instead\n",
    "- `ARRDELAY`: Will use the target variable instead\n",
    "- `ARRTIME`: Will use the CRSArrTime instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that are no longer needed\n",
    "cols_to_drop=['DEPTIME','ARRDELAY','ARRTIME']\n",
    "df=df.drop(cols_to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The dataset now has',df.shape[0],'rows and',df.shape[1],'columns.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue with further data preparation steps, we will first split our dataset into a training and test set using an 80/20 split. We will use our training data to develop our model and our test set to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X = df.drop(target,axis=1)\n",
    "y = df[target]\n",
    "\n",
    "# # 80/20 train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The training dataset has',X_train.shape[0],'rows and',X_train.shape[1],'columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The test dataset has',X_test.shape[0],'rows and',X_test.shape[1],'columns.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "\n",
    "Most machine learning algorithms cannot work with missing values. Therefore, it is important to identify any missing values and then to impute (i.e., replace) any missing values in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage of missing values and show top 10 columns with most missing values\n",
    "percent_missing = X_train.isna().sum() * 100 / len(X_train)\n",
    "percent_missing.sort_values(ascending=False, inplace=True)\n",
    "percent_missing[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values, so no imputation is required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Time Values\n",
    "We will convert the timestamp values in `CRSDepTime`,`WheelsOff`, and `CRSArrTime` to a categorical value split into 4 quarters of the day. We will split `CRSElapsedTime` into 30 min segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time values into categorical values\n",
    "X_train['CRSDEPTIME'] = np.ceil(X_train['CRSDEPTIME']/600).apply(int)\n",
    "X_train['WHEELSOFF'] = np.ceil(X_train['WHEELSOFF']/600).apply(int) \n",
    "X_train['CRSARRTIME'] = np.ceil(X_train['CRSARRTIME']/600).apply(int)\n",
    "X_train['CRSELAPSEDTIME']=np.ceil(X_train['CRSELAPSEDTIME']/30).apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "We will scale our continuous numerical columns: `AirTime`,`TaxiOut`,`DepDelay`.\n",
    "\n",
    "Most machine learning algorithms do not perform well when the numerical attributes in the data have very different scales (i.e., large range of possible values). It is important to make sure that the numerical features in our dataset have similar scales. A standardized value z of an input feature x is calculated as:\n",
    "\n",
    "z = (x - u) / s\n",
    "\n",
    "where u is the mean of the training samples and s is the standard deviation of the training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale continuous numerical columns\n",
    "cols_to_scale=['AIRTIME','TAXIOUT','DEPDELAY']\n",
    "scaler = ColumnTransformer(\n",
    "[('scaler', StandardScaler(),cols_to_scale)],\n",
    "remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_scaler = scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply standardization and return a dataframe\n",
    "col_names = X_train.columns.tolist()\n",
    "X_train_trsf = pd.DataFrame(trained_scaler.transform(X_train))\n",
    "X_train_trsf.columns=cols_to_scale + [col for col in col_names if col not in cols_to_scale]\n",
    "X_train_trsf.index=X_train.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hands-On Exercise 1\n",
    "\n",
    "While performing data transformations, it is always important to validate that the transformations were performed correctly.\n",
    "\n",
    "**Using pandas DataFrame .head() and/or .tail() method, visually validate that our feature scaling was performed correctly.**\n",
    "\n",
    "\n",
    "**Hint**: The syntax will be similar to `df.head()` or `df.tail()`, where `df` is the name of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values in `UNIQUECARRIER`, `ORIGIN`, and `DEST` need to be converted into categorical columns\n",
    "\n",
    "Most machine learning algorithms prefer to work with numbers rather than text values. A common solution is to create one binary attribute per category: one attribute equal to 1 when the category is “LAX” (and 0 otherwise), another attribute equal to 1 when the category is “JFK” (and 0 otherwise), and so on. This is called one-hot encoding, because only one attribute will be equal to 1 (hot), while the others will be 0 (cold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of unique values in each of these columns\n",
    "print('Number of unique values in UNIQUECARRIER:',X_train_trsf['UNIQUECARRIER'].nunique())\n",
    "print('Number of unique values in ORIGIN:',X_train_trsf['ORIGIN'].nunique())\n",
    "print('Number of unique values in DEST:',X_train_trsf['DEST'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, we can't apply one-hot encoding directly to these columns as we would be adding >750 new columns! Instead we will select the top 10 most common occurences in these columns as our labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the top 10 occurences in each column\n",
    "top_10_airlines = X_train_trsf['UNIQUECARRIER'].value_counts().index[:10].tolist()\n",
    "top_10_origin = X_train_trsf['ORIGIN'].value_counts().index[:10].tolist()\n",
    "top_10_dest = X_train_trsf['DEST'].value_counts().index[:10].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace values not in the top 10 list with \"Other\"\n",
    "X_train_trsf['UNIQUECARRIER'] = np.where(~X_train_trsf['UNIQUECARRIER'].isin(top_10_airlines), \n",
    "                                                       'Other_Airline', X_train_trsf['UNIQUECARRIER'])\n",
    "X_train_trsf['ORIGIN'] = np.where(~X_train_trsf['ORIGIN'].isin(top_10_origin), \n",
    "                                                       'Other_Origin', X_train_trsf['ORIGIN'])\n",
    "X_train_trsf['DEST'] = np.where(~X_train_trsf['DEST'].isin(top_10_dest), \n",
    "                                                       'Other_Dest', X_train_trsf['DEST'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm it worked\n",
    "print('Number of unique values in UNIQUECARRIER:',X_train_trsf['UNIQUECARRIER'].nunique())\n",
    "print('Number of unique values in ORIGIN:',X_train_trsf['ORIGIN'].nunique())\n",
    "print('Number of unique values in DEST:',X_train_trsf['DEST'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_trsf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to perform one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding\n",
    "cols_to_encode = ['UNIQUECARRIER','ORIGIN','DEST']\n",
    "encoder = ColumnTransformer(transformers=[\n",
    "    ('encoder',OneHotEncoder(handle_unknown='ignore'),cols_to_encode)],\n",
    "    remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_encoder = encoder.fit(X_train_trsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_trsf = trained_encoder.transform(X_train_trsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The training now dataset has',X_train_trsf.shape[0],'rows and',X_train_trsf.shape[1],'columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature names after fitting the encoder - this will be used to determine the model's feature importance\n",
    "feature_names=trained_encoder.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to apply the same transformations to our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time to categorical variable\n",
    "X_test['CRSDEPTIME'] = np.ceil(X_test['CRSDEPTIME']/600).apply(int)\n",
    "X_test['WHEELSOFF'] = np.ceil(X_test['WHEELSOFF']/600).apply(int) \n",
    "X_test['CRSARRTIME'] = np.ceil(X_test['CRSARRTIME']/600).apply(int)\n",
    "X_test['CRSELAPSEDTIME']=np.ceil(X_test['CRSELAPSEDTIME']/30).apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "col_names = X_test.columns.tolist()\n",
    "X_test_trsf = pd.DataFrame(trained_scaler.transform(X_test))\n",
    "X_test_trsf.columns=cols_to_scale + [col for col in col_names if col not in cols_to_scale]\n",
    "X_test_trsf.index=X_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace values not in the top 10 list with \"Other\"\n",
    "X_test_trsf['UNIQUECARRIER'] = np.where(~X_test_trsf['UNIQUECARRIER'].isin(top_10_airlines), \n",
    "                                                       'Other_Airline', X_test_trsf['UNIQUECARRIER'])\n",
    "X_test_trsf['ORIGIN'] = np.where(~X_test_trsf['ORIGIN'].isin(top_10_origin), \n",
    "                                                       'Other_Origin', X_test_trsf['ORIGIN'])\n",
    "X_test_trsf['DEST'] = np.where(~X_test_trsf['DEST'].isin(top_10_dest), \n",
    "                                                       'Other_Dest', X_test_trsf['DEST'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding\n",
    "X_test_trsf = trained_encoder.transform(X_test_trsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The test dataset has',X_test_trsf.shape[0],'rows and',X_test_trsf.shape[1],'columns.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Suppose you ask a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert’s answer. This is called the *wisdom of the crowd*. Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of predictors is called an *ensemble*; thus, this technique is called *Ensemble Learning*. For example, you can train a group of Decision Tree classifiers, each on a different random subset of the training set. To make predictions, you just obtain the predictions of all individual trees, then predict the class that gets the most votes. Such an ensemble of Decision Trees is called a **Random Forest**, and despite its simplicity, this is one of the most powerful Machine Learning algorithms available today.\n",
    "\n",
    "We will train a random forest classifer using the default values and setting the `class_weight` parameter to \"balanced\" due to our imbalanced training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model\n",
    "rf = RandomForestClassifier(random_state=42,class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "trained_rf = rf.fit(X_train_trsf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = trained_rf.predict(X_test_trsf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the F1 score (harmonic mean of Precision and Recall) as our metric for evaluating model performance. An F1-Score of 1.0, indicates perfect precision and recall, and the lowest possible value is 0, if either the precision or the recall is zero.\n",
    "\n",
    "Precision is the number of true positive results divided by the number of all positive results, including those not identified correctly, and the recall is the number of true positive results divided by the number of all samples that should have been identified as positive.\n",
    "\n",
    "The F1 score can be expressed as\n",
    "\n",
    "$\\frac{TP}{TP +\\frac{1}{2}(FP+FN)}$,\n",
    "\n",
    "where TP is the number of true positives, FP is the number of false positives, and FN is the number of false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's evaluate the model performance\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "print('Model F1 Score: %.3f' % f1_score(y_test,y_pred))\n",
    "\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n",
    "            fmt='.2%', cmap='Blues',xticklabels=['On Time','Delayed'],yticklabels=['On Time','Delayed'])\n",
    "plt.title('Random Forest')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look at the features the model has deemed as important for determining if a flight will be delayed or on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances\n",
    "feature_importances=pd.DataFrame(trained_rf.feature_importances_.flatten(), index=trained_encoder.get_feature_names(), \n",
    "                                 columns=['Importance'])\n",
    "feature_importances.sort_values(by='Importance',ascending=False).head(15).plot.bar();\n",
    "plt.ylabel('Feature Importance')\n",
    "plt.title('Random Forest Classifier Feature Importances');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Deployment Assets\n",
    "\n",
    "We now need to save our deployment assets to the shared filesystem. They include:\n",
    "- top 10 carriers, origin, and destinations\n",
    "- trained scalar\n",
    "- trained one-hot encoder\n",
    "- trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our deployment assets to the shared filesystem\n",
    "dump(top_10_airlines,'../../../PYUDF/top_10_airlines.joblib')\n",
    "dump(top_10_origin,'../../../PYUDF/top_10_origin.joblib')\n",
    "dump(top_10_dest,'../../../PYUDF/top_10_dest.joblib')\n",
    "dump(trained_scaler,'../../../PYUDF/trained_scaler.joblib')\n",
    "dump(trained_encoder,'../../../PYUDF/trained_encoder.joblib')\n",
    "dump(trained_rf,'../../../PYUDF/trained_rf.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Deploying the Pipeline to Db2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to deploy our model to Db2 for in-database scoring.\n",
    "\n",
    "We will:\n",
    "1. Create a UDF source file\n",
    "2. Register our Python UDF\n",
    "3. Call our UDF with a SQL statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our UDTF (User Defined Table Function) has the following sections:\n",
    "- **Initialization**: Define any static variables, load our deployment assets (e.g. model), and prepare for batch scoring\n",
    "- **Data Preparation**: Perform the same data preparation steps we performed during model development\n",
    "- **Model Scoring & Output**: Call our model to make predictions and return the results\n",
    "\n",
    "Our UDTF will output the following:\n",
    "- `DATE`: The date of the flight \n",
    "- `ORIGIN`: The origin airport\n",
    "- `DEST`: The destination airport\n",
    "- `CARRIER`: The airline code\n",
    "- `CRSDEPTIME`: The scheduled departure time\n",
    "- `CRSARRTIME`: The scheduled arrival time \n",
    "- `PREDICTION`: The prediction (0 = on time, 1 = delayed)\n",
    "- `PROB_DELAYED`: The predicted probability that the flight will be delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](media/udfdeployment.png)\n",
    "\n",
    "**Figure 2**: This figure depicts how a UDF source file is created from the original pipeline code developed during the model development stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../../../PYUDF/myUDF.py\n",
    "\n",
    "#Imports\n",
    "import nzae\n",
    "import pandas as pd\n",
    "from joblib import load\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class full_pipeline(nzae.Ae):\n",
    "    def _runUdtf(self):\n",
    "        #####################\n",
    "        ### INITIALIZATON ###\n",
    "        #####################\n",
    "        \n",
    "        # Define static variables\n",
    "        input_cols = ['YEAR','QUARTER', 'MONTH',\n",
    "                      'DAYOFMONTH', 'DAYOFWEEK','UNIQUECARRIER',\n",
    "                      'ORIGIN', 'DEST', 'CRSDEPTIME',\n",
    "                      'DEPDELAY', 'DEPDEL15','TAXIOUT','WHEELSOFF',\n",
    "                      'CRSARRTIME', 'CRSELAPSEDTIME', 'AIRTIME', 'DISTANCEGROUP']\n",
    "        cols_to_scale=['AIRTIME','TAXIOUT','DEPDELAY']\n",
    "        cols_to_encode = ['UNIQUECARRIER','ORIGIN','DEST']\n",
    "\n",
    "        # Load deployment assets\n",
    "        top_10_airlines= load('home/db2inst1/PYUDF/top_10_airlines.joblib')\n",
    "        top_10_origin=load('home/db2inst1/PYUDF/top_10_origin.joblib')\n",
    "        top_10_dest= load('home/db2inst1/PYUDF/top_10_dest.joblib')\n",
    "        trained_scaler = load('home/db2inst1/PYUDF/trained_scaler.joblib')\n",
    "        trained_encoder = load('home/db2inst1/PYUDF/trained_encoder.joblib')\n",
    "        trained_rf = load('home/db2inst1/PYUDF/trained_rf.joblib')\n",
    "     \n",
    "        # Collect rows into a single batch\n",
    "        batchsize = 10000\n",
    "        rownum = 0\n",
    "        row_list = []\n",
    "        for row in self:\n",
    "            row_list.append(row)\n",
    "            rownum = rownum+1         \n",
    "\n",
    "            if rownum==batchsize:\n",
    "                ########################\n",
    "                ### DATA PREPARATION ###\n",
    "                ########################\n",
    "                \n",
    "                # Collect the rows into a dataframe\n",
    "                df = pd.DataFrame(row_list, columns=input_cols)\n",
    "\n",
    "\n",
    "                # Save the original datestring (yyyymmdd), ORIGIN, DEST, UNIQUECARRIER, CRSDEPTIME, CRSARRTIME\n",
    "                # to return with the model prediction\n",
    "                dates = [int(year + month + day) for year, month, day in \n",
    "                         zip(map(str,list(df['YEAR'])), map(str,[\"%02d\" %i for i in list(df['MONTH'])]), \n",
    "                             map(str,[\"%02d\" %i for i in list(df['DAYOFMONTH'])]))]\n",
    "                origins = list(df['ORIGIN'])\n",
    "                dests = list(df['DEST'])\n",
    "                carriers = list(df['UNIQUECARRIER'])\n",
    "                deptimes = list(df['CRSDEPTIME'])\n",
    "                arrtimes = list(df['CRSARRTIME'])\n",
    "\n",
    "                \n",
    "                # Fill any missing values\n",
    "                df['DEPDELAY']=df['DEPDELAY'].fillna(0)\n",
    "                df['DEPDEL15']=df['DEPDEL15'].fillna(0)\n",
    "\n",
    "                # Convert time values to categorical variable\n",
    "                df['CRSDEPTIME'] = np.ceil(df['CRSDEPTIME']/600).apply(int)\n",
    "                df['WHEELSOFF'] = np.ceil(df['WHEELSOFF']/600).apply(int) \n",
    "                df['CRSARRTIME'] = np.ceil(df['CRSARRTIME']/600).apply(int)\n",
    "                df['CRSELAPSEDTIME']=np.ceil(df['CRSELAPSEDTIME']/30).apply(int)\n",
    "\n",
    "                # Feature scaling\n",
    "                col_names = df.columns.tolist()\n",
    "                df_trsf = pd.DataFrame(trained_scaler.transform(df))\n",
    "                df_trsf.columns=cols_to_scale + [col for col in col_names if col not in cols_to_scale]\n",
    "                df_trsf.index=df.index\n",
    "\n",
    "                # Replace values not in the top 10 list with \"Other\"\n",
    "                df_trsf['UNIQUECARRIER'] = np.where(~df_trsf['UNIQUECARRIER'].isin(top_10_airlines), \n",
    "                                                                       'Other_Airline', df_trsf['UNIQUECARRIER'])\n",
    "                df_trsf['ORIGIN'] = np.where(~df_trsf['ORIGIN'].isin(top_10_origin), \n",
    "                                                                       'Other_Origin', df_trsf['ORIGIN'])\n",
    "                df_trsf['DEST'] = np.where(~df_trsf['DEST'].isin(top_10_dest), \n",
    "                                                                       'Other_Dest', df_trsf['DEST'])\n",
    "\n",
    "                # One-hot encoding\n",
    "                df_trsf = trained_encoder.transform(df_trsf)\n",
    "\n",
    "                ##############################\n",
    "                ### MODEL SCORING & OUTPUT ###\n",
    "                ##############################\n",
    "                \n",
    "                # Call model to make prediction\n",
    "                predictions = trained_rf.predict(df_trsf)\n",
    "                \n",
    "                # Calculate probability that flight will be delayed\n",
    "                probability_delayed = trained_rf.predict_proba(df_trsf)[:,1]\n",
    "                \n",
    "                # Return the result\n",
    "                for x in range(predictions.shape[0]):\n",
    "                    self.output(int(dates[x]),str(origins[x]),str(dests[x]),str(carriers[x]),\n",
    "                                int(deptimes[x]),int(arrtimes[x]),int(predictions[x]),\n",
    "                                float(probability_delayed[x]))\n",
    "\n",
    "                row_list = []\n",
    "                rownum = 0\n",
    "                \n",
    "        self.done()\n",
    "full_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to register our UDTF. We will provide it a name `FLIGHT_PREDICTER`, define the input datatypes, the definition of the table it outputs (i.e. column names and datatypes), and the path to our source file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE OR REPLACE FUNCTION \n",
    "FLIGHT_PREDICTER(SMALLINT,SMALLINT,SMALLINT,SMALLINT,SMALLINT,VARCHAR(8),VARCHAR(3),\n",
    "              VARCHAR(3),SMALLINT,SMALLINT,SMALLINT,SMALLINT,SMALLINT,SMALLINT,SMALLINT,\n",
    "              SMALLINT,SMALLINT) \n",
    "RETURNS TABLE (DATE INTEGER,ORIGIN VARCHAR(3), DEST VARCHAR(3), CARRIER VARCHAR(3), \n",
    "CRSDEPTIME SMALLINT, CRSARRTIME SMALLINT,PREDICTION SMALLINT,PROB_DELAYED DOUBLE)\n",
    "LANGUAGE PYTHON PARAMETER STYLE NPSGENERIC  FENCED  NOT THREADSAFE  NO FINAL CALL  DISALLOW PARALLEL  NO DBINFO  \n",
    "DETERMINISTIC NO EXTERNAL ACTION CALLED ON NULL INPUT  \n",
    "NO SQL EXTERNAL NAME 'home/db2inst1/PYUDF/myUDF.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our UDTF source file created and our function registered, we can now call the Python UDTF to make predictions. We will create a table `ONTIME.PREDICTIONS` to store the results, and insert the output from our UDTF into that table. Recall that we will be making predictions on new flight data from 2020 (i.e., the model is using historical data to make predictions on future events)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS ONTIME.PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE TABLE ONTIME.PREDICTIONS(DATE INTEGER,\n",
    "                                ORIGIN VARCHAR(3), \n",
    "                                DEST VARCHAR(3),\n",
    "                                CARRIER VARCHAR(3),\n",
    "                                CRSDEPTIME SMALLINT, \n",
    "                                CRSARRTIME SMALLINT,\n",
    "                                PREDICTION SMALLINT,\n",
    "                                PROB_DELAYED DOUBLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%sql\n",
    "INSERT INTO ONTIME.PREDICTIONS(DATE,ORIGIN,DEST,CARRIER,CRSDEPTIME,CRSARRTIME,PREDICTION,PROB_DELAYED)\n",
    "SELECT f.* from ONTIME.TEST i,\n",
    "TABLE(FLIGHT_PREDICTER(i.YEAR, i.QUARTER, i.MONTH, \n",
    "                i.DAYOFMONTH, i.DAYOFWEEK,i.UNIQUECARRIER,\n",
    "                i.ORIGIN, i.DEST, i.CRSDEPTIME,\n",
    "                i.DEPDELAY, i.DEPDEL15,i.TAXIOUT,i.WHEELSOFF,\n",
    "                i.CRSARRTIME, i.CRSELAPSEDTIME, i.AIRTIME, i.DISTANCEGROUP)) f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can query the `ONTIME.PREDICTIONS` table to look at the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = %sql SELECT * FROM ONTIME.PREDICTIONS\n",
    "result = pd.DataFrame(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Lets look at the first 10 rows of our UDTF's output\n",
    "result.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Build and deploy your own UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will deploy your own ML classification model. We will provide the steps to build and deploy the model, including any necessary data transformation steps. You will be responsible for filling the UDF template provided during the model deployment phase.\n",
    "\n",
    "The dataset used is the famous IRIS dataset - it contains samples from each of three species of *Iris* flowers. (*Iris setosa*, *Iris virginica*, and *Iris versicolor*). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "import numpy as np\n",
    "\n",
    "# Model building\n",
    "from sklearn.model_selection import train_test_split # Train-test split\n",
    "from sklearn.impute import SimpleImputer # Missing values imputation\n",
    "from sklearn.compose import ColumnTransformer # For applying transformation objects\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score # Model evaluation\n",
    "from joblib import dump #Saving deployment assets\n",
    "\n",
    "# Plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(rc={'figure.figsize':(12,8)})\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load the Db2 magic functions and make a connection to our Db2 database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%run db2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run connectiondb2ml.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the training data\n",
    "query = %sql SELECT * FROM ONTIME.IRIS_TRAIN\n",
    "df = pd.DataFrame(query)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "target = \"CLASS\"\n",
    "X = df.drop(target,axis=1)\n",
    "y = df[target]\n",
    "\n",
    "# Our dataset size is small, so we will do a 70/30 train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The training dataset has',X_train.shape[0],'rows and',X_train.shape[1],'columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The test dataset has',X_test.shape[0],'rows and',X_test.shape[1],'columns.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check for missing values in our training and test sets, and perform imputation if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of missing values in the training set\n",
    "number_of_missing_vals = X_train.isna().sum()\n",
    "number_of_missing_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's perform missing value imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an imputer to perform missing value imputation\n",
    "imputer= SimpleImputer(strategy='mean')\n",
    "trained_imputer = imputer.fit(X_train)\n",
    "\n",
    "# Transform the training set and return as a dataframe\n",
    "X_train_trsf = pd.DataFrame(trained_imputer.transform(X_train),\n",
    "                            columns=['SEPAL_LENGTH','SEPAL_WIDTH','PETAL_LENGTH','PETAL_WIDTH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's repeat these steps for our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look if our test set has any missing values\n",
    "number_of_missing_vals = X_test.isna().sum()\n",
    "number_of_missing_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's transform our test set before we make any predictions\n",
    "X_test_trsf = trained_imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation\n",
    "\n",
    "We will train a simple decision tree model to predict the type of flower based on the sepal and petal length and width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train a simple decision tree model\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "trained_dt = dt.fit(X_train_trsf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next will make predictions on our test set and evaluate the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's evaluate the performance of the model\n",
    "y_pred = trained_dt.predict(X_test_trsf)\n",
    "\n",
    "# Plot confusion matrix\n",
    "print('Model F1 Score: %.3f' % f1_score(y_test,y_pred,average='weighted'))\n",
    "\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n",
    "            fmt='.2%', cmap='Blues',xticklabels=['Iris-setosa','Iris-virginica','Iris-versicolor'],\n",
    "            yticklabels=['Iris-setosa','Iris-virginica','Iris-versicolor'])\n",
    "plt.title('Decision Tree')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Deployment Assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are satisfied with our model, we will save the trained imputer and trained model to the shared filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our model and scalar\n",
    "dump(trained_imputer,'../../../PYUDF/trained_imputer.joblib')\n",
    "dump(trained_dt,'../../../PYUDF/trained_dt.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Deployment\n",
    "\n",
    "Here, we have provided a UDF template for you to fill out.\n",
    "\n",
    "1. Add some code to load in the trained imputer and scalar (**Hint**: the path where the deployment assets are stored is `home/db2inst1/PYUDF/<file_name>`)\n",
    "2. Add some code to perform the missing value imputation on the data\n",
    "3. Add some code to make predictions with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../../../PYUDF/exerciseUDF.py\n",
    "\n",
    "#Imports\n",
    "import nzae\n",
    "import pandas as pd\n",
    "from joblib import load\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class full_pipeline(nzae.Ae):\n",
    "    def _runUdtf(self):\n",
    "        #####################\n",
    "        ### INITIALIZATON ###\n",
    "        #####################\n",
    "        \n",
    "        # Define static variable\n",
    "        input_cols=['SEPAL_LENGTH','SEPAL_WIDTH','PETAL_LENGTH','PETAL_WIDTH']\n",
    "        # Load deployment assets\n",
    "        \n",
    "        trained_imputer = #### YOUR CODE HERE ####\n",
    "        trained_dt = #### YOUR CODE HERE ####\n",
    "     \n",
    "        # Collect rows into a single batch\n",
    "        batchsize = 110\n",
    "        rownum = 0\n",
    "        row_list = []\n",
    "        for row in self:\n",
    "            row_list.append(row)\n",
    "            rownum = rownum+1         \n",
    "\n",
    "            if rownum==batchsize:\n",
    "                ########################\n",
    "                ### DATA PREPARATION ###\n",
    "                ########################\n",
    "                \n",
    "                # Collect the rows into a dataframe\n",
    "                df = pd.DataFrame(row_list, columns=input_cols)\n",
    "\n",
    "                # Feature imputation\n",
    "                df_trsf = pd.DataFrame(#### YOUR CODE HERE ####\n",
    "                                       ,columns=input_cols)\n",
    "\n",
    "\n",
    "                ##############################\n",
    "                ### MODEL SCORING & OUTPUT ###\n",
    "                ##############################\n",
    "                \n",
    "                # Call model to make prediction\n",
    "                predictions = #### YOUR CODE HERE ####\n",
    "                \n",
    "                \n",
    "                # Return the result\n",
    "                for x in range(predictions.shape[0]):\n",
    "                    self.output(float(df_trsf['SEPAL_LENGTH'][x]),\n",
    "                                float(df_trsf['SEPAL_WIDTH'][x]),\n",
    "                                float(df_trsf['PETAL_LENGTH'][x]),\n",
    "                                float(df_trsf['PETAL_WIDTH'][x]),\n",
    "                                str(predictions[x]))\n",
    "\n",
    "                row_list = []\n",
    "                rownum = 0\n",
    "                \n",
    "        self.done()\n",
    "full_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the following cells to check your work. **Hint:** If you encounter errors, the UDF log files can be found in the `sqllib/db2dump/pythonUDX/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE OR REPLACE FUNCTION \n",
    "MY_UDF(DOUBLE,DOUBLE,DOUBLE,DOUBLE) \n",
    "RETURNS TABLE (SEPAL_LENGTH DOUBLE,\n",
    "    SEPAL_WIDTH DOUBLE,\n",
    "    PETAL_LENGTH DOUBLE,\n",
    "    PETAL_WIDTH DOUBLE,\n",
    "    PREDICTION VARCHAR(15))\n",
    "LANGUAGE PYTHON PARAMETER STYLE NPSGENERIC  FENCED  NOT THREADSAFE  NO FINAL CALL  DISALLOW PARALLEL  NO DBINFO  \n",
    "DETERMINISTIC NO EXTERNAL ACTION CALLED ON NULL INPUT  \n",
    "NO SQL EXTERNAL NAME 'home/db2inst1/PYUDF/exerciseUDF.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS ONTIME.IRIS_PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE TABLE ONTIME.IRIS_PREDICTIONS(\n",
    "    SEPAL_LENGTH DOUBLE,\n",
    "    SEPAL_WIDTH DOUBLE,\n",
    "    PETAL_LENGTH DOUBLE,\n",
    "    PETAL_WIDTH DOUBLE,\n",
    "    PREDICTION VARCHAR(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%sql\n",
    "INSERT INTO ONTIME.IRIS_PREDICTIONS(SEPAL_LENGTH,SEPAL_WIDTH,PETAL_LENGTH,PETAL_WIDTH,PREDICTION)\n",
    "SELECT f.* from ONTIME.IRIS_TEST i,\n",
    "TABLE(MY_UDF(i.SEPAL_LENGTH,i.SEPAL_WIDTH,i.PETAL_LENGTH,i.PETAL_WIDTH)) f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = %sql SELECT * FROM ONTIME.IRIS_PREDICTIONS\n",
    "result = pd.DataFrame(query)\n",
    "result.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Video: Deploying a Machine Learning Model Trained on IBM Cloud Pak for Data into Db2](https://video.ibm.com/recorded/129516812)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
